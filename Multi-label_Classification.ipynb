{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Multi-label Classification for abstract dataset**"
      ],
      "metadata": {
        "id": "wsHVxRj7Vu36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre experiment phase:**\n",
        "- 1) Preliminary analysis\n",
        "\n",
        "**Experiment phase:**\n",
        "- 2) Preprocessing\n",
        "- 3) Metrics selection\n",
        "- 4) Model training and evaluation\n",
        "- 5) Hyperparameter tuning and best model selection\n",
        "\n",
        "**Post experiment phase:**\n",
        "- 6) Evaluation on test data"
      ],
      "metadata": {
        "id": "_wvrsPgxVjPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1) Preliminary data analysis:**\n",
        "\n"
      ],
      "metadata": {
        "id": "km_bnjUKZELV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Print data\n",
        "- Visualization\n",
        "- Number of samples\n",
        "- Number of features\n",
        "- Number of classes\n",
        "- Number of samples for each class"
      ],
      "metadata": {
        "id": "zlQLbBL2K7Ks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qijJ3KtbUpGg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get raw data\n",
        "raw_data = pd.read_pickle('/content/ass2.pickle')\n",
        "X_train = raw_data['train'].iloc[:,0:42] # all features\n",
        "y_train = raw_data['train']['target'] # target\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.fit(y_train)"
      ],
      "metadata": {
        "id": "D2s8MW0VOB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_data)"
      ],
      "metadata": {
        "id": "Z7f1Yc02OGdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of data distribution for first impressions**"
      ],
      "metadata": {
        "id": "tYaO78TcLsg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution of samples per class**"
      ],
      "metadata": {
        "id": "oIpWduwxYBgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of samples for each class\n",
        "class_counts = y_train.value_counts()\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar(class_counts.index, class_counts.values)\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Distribution of Samples by Class')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "\n",
        "# Set the x-axis limits\n",
        "plt.xlim(0, len(class_counts))\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NVJTbKuwXB8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution of features**"
      ],
      "metadata": {
        "id": "eXAz7rPpYHNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Iterate over each class\n",
        "for label in y_train.unique():\n",
        "    # Select data for the current class\n",
        "    data = X_train[y_train == label].values.flatten()\n",
        "\n",
        "    # Plot the histogram\n",
        "    plt.hist(data, bins=20, alpha=0.5, label=f'Class {label}')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Distribution of Features by Class')\n",
        "plt.xlabel('Feature Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s1TMEFo6LstH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that most of the samples belong to class 2, second most samples  to class 1 and class 0 has less samples.\n",
        "Thus, from first impression we conclude that the data is imbalanced.\n",
        "\n",
        "Now get the exact numbers of distribution:"
      ],
      "metadata": {
        "id": "frVX8hTTBHiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Total number of (samples, features) in train-set is:', X_train.shape)\n",
        "print('Labels of dataset are:', label_encoder.classes_ )\n",
        "print('Number of samples of each lable is:')\n",
        "print(y_train.value_counts(), '\\n')\n",
        "\n",
        "X_dev = raw_data['dev'].iloc[:,0:42]\n",
        "y_dev = raw_data['dev']['target']\n",
        "print('Total number of (samples, features) in dev-set is:', X_dev.shape)\n",
        "print('Number of samples of each lable is:')\n",
        "print(y_dev.value_counts(), '\\n')\n",
        "\n",
        "X_test = raw_data['test'].iloc[:,0:42]\n",
        "y_test = raw_data['test']['target']\n",
        "print('Total number of (samples, features) in test-set is:', X_test.shape)\n",
        "print(y_test.value_counts(), '\\n')"
      ],
      "metadata": {
        "id": "UzhfcrW-vywZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Preliminary data analysis shows that dataset is imbalanced.*"
      ],
      "metadata": {
        "id": "w8lt42Zp9MPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2)Preprocessing**\n",
        "\n"
      ],
      "metadata": {
        "id": "mR1rJWiOVrGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- balancing data\n",
        "- Standard scaling\n",
        "\n",
        "*Explanation:*\n",
        "> Logistic Regression, KNN and SVM are sensitive to imbalanced and not standardized data thus, we balance and standardize X_train and X_dev.\n",
        "\n",
        "\n",
        "> Tree classifiers aren't sensitive to imbalanced and not stadardized data."
      ],
      "metadata": {
        "id": "uJY40V3ELAZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing the data**"
      ],
      "metadata": {
        "id": "ik9qIXoY6tcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "# Define the undersampler\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "\n",
        "# Fit and transform the training set\n",
        "X_train_balanced, y_train_balanced = undersampler.fit_resample(X_train, y_train)\n",
        "print('Now after balancing the number of samples of each lable in train-set is: \\n')\n",
        "print(y_train_balanced.value_counts(), '\\n')\n",
        "print('Total (samples, features) in train-set after balancing:',  X_train_balanced.shape)"
      ],
      "metadata": {
        "id": "pnb_JxGRhglx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standard scaling**"
      ],
      "metadata": {
        "id": "Y3RBzqCR633-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_transformed = scaler.transform(X_train)\n",
        "scaler.fit(X_train_balanced)\n",
        "X_train_transformed_balanced = scaler.transform(X_train_balanced)\n",
        "scaler.fit(X_dev)\n",
        "X_dev_transformed = scaler.transform(X_dev)"
      ],
      "metadata": {
        "id": "3kq3gej066A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3) Metrics selection**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RnV0zHuDV9s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll evaluate due to following metrices:\n",
        "- accuracy\n",
        "- weighted F1-score\n",
        "- cross validation score\n",
        "- classification report (precision/recall and F1-score)\n",
        "- confusion matrix\n",
        "\n",
        "*Explanation:*\n",
        "> Since the data set is named only \"generally\" as feature names aren't named specifically but f1,f2,...,f42 and class labels are named as class 0,1,2 and not specific, we don't really know the datas contents and issues. Thus we don't know whether a high precision or a low recall is desirable in prediction.\n",
        "Hence, concluding that we'll use basically the harmonic F1-score of precision/recall for model evaluating. Specifically using F1-score weighted average because test data is imbalanced as well."
      ],
      "metadata": {
        "id": "C594ru0yLGHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, f1_score,accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold"
      ],
      "metadata": {
        "id": "K3jvUVSSgJJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4) Model training and evaluation**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCSBUhGxf-ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll train the following models:\n",
        "\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- GBDT - Gradient Boost Decision Tree\n",
        "- Logistic Regression\n",
        "- K-nearest Neighbors\n",
        "\n",
        "Hence the dataset is provided with labels/targets we won't train a clustering models."
      ],
      "metadata": {
        "id": "y39iFQjiLItm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "BMqo3_1336br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_sensitive_models = [\n",
        "          ('Decision Tree',DecisionTreeClassifier(random_state=42)),\n",
        "          ('Random Forrest', RandomForestClassifier(random_state=42)),\n",
        "           ('Gradien Boost Decision Tree',GradientBoostingClassifier(learning_rate=1, random_state=42)),\n",
        "        ]\n",
        "\n",
        "sensitive_models = [\n",
        "          ('Logistic Regression', LogisticRegression(max_iter=1000)),\n",
        "          ('KNN', KNeighborsClassifier())\n",
        "        ]\n",
        "\n",
        "target_names = ['0', '1', '2']\n",
        "\n",
        "def eval_model(model,cv_scores, x_eval, y_eval, accuracies):\n",
        "  y_dev_pred = clf.predict(x_eval)\n",
        "  accuracy = accuracy_score(y_eval, y_dev_pred)\n",
        "  if (accuracies.get('model') is None) or accuracy > accuracies.get('model'):\n",
        "    accuracies[model] = accuracy\n",
        "  #print results\n",
        "  print(f'Classifier                       : {model}')\n",
        "  print(f'Accuracy                         : {accuracy}')\n",
        "  print(f'Weighted F1-score is             : {f1_score(y_eval, y_dev_pred ,average= \"weighted\")}')\n",
        "  print(f'cross-validation mean accuracies : {cv_scores.mean()}')\n",
        "  print(f'classification report\\n {classification_report(y_eval, y_dev_pred, target_names=target_names)}')\n",
        "  print(f'confusion matrix\\n {confusion_matrix(y_eval, y_dev_pred)}', '\\n\\n')"
      ],
      "metadata": {
        "id": "nNGwjCEQCV7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1) Training Decision Tree, Random Forrest and Gradient Boost with imbalanced data**"
      ],
      "metadata": {
        "id": "AWQEBKRug80F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies_non_sensitive_models_imbalanced = {}\n",
        "#train with 5-cross validation\n",
        "print('Training and evaluating models with cross validation \\n\\n')\n",
        "for _,model in non_sensitive_models:\n",
        "        clf = model.fit(X_train, y_train)\n",
        "        cv_scores = cross_val_score(clf, X_train, y_train,scoring='accuracy', cv=5, n_jobs=-1)\n",
        "        eval_model(clf,cv_scores, X_dev, y_dev, accuracies_non_sensitive_models_imbalanced)\n",
        "\n",
        "print(f'Accuracies are: {accuracies_non_sensitive_models_imbalanced}\\n')"
      ],
      "metadata": {
        "id": "cqGdH73lgFGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2) Training Decision Tree, Random Forrest and Gradient Boost with balanced data**"
      ],
      "metadata": {
        "id": "k2fDJUg41NtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies_non_sensitive_models_balanced={}\n",
        "#train with undersampled balanced data and 5-cross validation\n",
        "print('Training and evaluating models after undersampling and with cross validation \\n\\n')\n",
        "for _, model in non_sensitive_models:\n",
        "        clf = model.fit(X_train_balanced, y_train_balanced)\n",
        "        cv_scores = cross_val_score(clf, X_train_balanced, y_train_balanced ,scoring='accuracy', cv=5, n_jobs=-1)\n",
        "        eval_model(clf, X_dev, y_dev,accuracies_non_sensitive_models_balanced)\n",
        "\n",
        "print(f'Accuracies are: {accuracies_non_sensitive_models_balanced}\\n')\n",
        "\n",
        "#train with repeated stratified 5-fold cross validation\n",
        "rep_strat_kfold = RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=42)\n",
        "print('Training and evaluating models with repeated stratified 5-fold cross validation \\n\\n')\n",
        "for _, model in sensitive_models:\n",
        "        clf = model.fit(X_train, y_train)\n",
        "        cv_scores = cross_val_score(clf, X_train, y_train ,scoring='accuracy', cv=rep_strat_kfold, n_jobs=-1)\n",
        "        eval_model(clf, X_train, y_dev,accuracies_non_sensitive_models_balanced)\n",
        "print(f'Accuracies are: {accuracies_non_sensitive_models_balanced}\\n')"
      ],
      "metadata": {
        "id": "aSzbwitr1Pa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the tree algorithms performed worse on the balanced data after undersampling, even when tree algorithms are generally not sensitive to imbalanced or balanced data. This may be a result of the \"nature\" of given data and altering the original data distribution may lead to decrease of accuracy which may be caused due to loss of information, class noise etc. after the undersampling."
      ],
      "metadata": {
        "id": "45nJ8dZvfuyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3) Training separately classifiers sensitive to imbalanced and not stadardized data.**"
      ],
      "metadata": {
        "id": "KOFukSvLjFRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies_sensitive_models_={}\n",
        "#train with undersampled balanced data and 5-cross validation\n",
        "print('Training and evaluating models after undersampling and with cross validation \\n\\n')\n",
        "for _, model in sensitive_models:\n",
        "        clf = model.fit(X_train_transformed_balanced, y_train_balanced)\n",
        "        cv_scores = cross_val_score(clf, X_train_transformed_balanced, y_train_balanced ,scoring='accuracy', cv=5, n_jobs=-1)\n",
        "        eval_model(clf, X_dev_transformed, y_dev,accuracies_sensitive_models_ , cv=5)\n",
        "print(f'Accuracies are: {accuracies_sensitive_models_}\\n')\n",
        "\n",
        "#train with repeated stratified 5-fold cross validation\n",
        "rep_strat_kfold = RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=42)\n",
        "print('Training and evaluating models with repeated stratified 5-fold cross validation \\n\\n')\n",
        "for _, model in sensitive_models:\n",
        "        clf = model.fit(X_train_transformed, y_train)\n",
        "        cv_scores = cross_val_score(clf, X_train_transformed_balanced, y_train_balanced ,scoring='accuracy', cv=rep_strat_kfold, n_jobs=-1)\n",
        "        eval_model(clf, X_dev_transformed, y_dev,accuracies_sensitive_models_)\n",
        "print(f'Accuracies are: {accuracies_sensitive_models_}\\n')"
      ],
      "metadata": {
        "id": "YFl0joYhjOIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that KNN and Logistic Regression both performed much better with repeated stratified 5-fold cross validation then with the undersampled data.\n",
        "This could be explained since repeated stratified k-fold cross validation:\n",
        "- provides a more comprehensive evaluation by considering multiple iterations of training and testing on different subsets of the data.\n",
        "- reduces the potential bias introduced by undersampling, where some information from the majority class may be lost.\n",
        "- allows the models to learn from a wider range of samples, including both majority and minority class instances, which can lead to better performance."
      ],
      "metadata": {
        "id": "BRzHCEnF-UUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5) Hyperparamter tuning and best model selection**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXhA-LqqrRJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.1) Tuning hyperparameters of best 2 models of section 4:**"
      ],
      "metadata": {
        "id": "3a4kWbhdAyn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:*\n",
        "\n",
        "> We found Random Forest Classifier and Gradien Boost Classifier performing best on dev-set. Thus, tuning hyperparameters of both, trying to improve performance on dev-set and finally choosing best classifier on dev-set."
      ],
      "metadata": {
        "id": "dN6-4xclLh4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_accuracies = [accuracies_non_sensitive_models_imbalanced, accuracies_non_sensitive_models_balanced, accuracies_sensitive_models_]\n",
        "def get_best_classifiers(overall_accuracies):\n",
        "    top_models = []\n",
        "    top_accuracies = [0.0, 0.0]\n",
        "\n",
        "    for dictionary in overall_accuracies:\n",
        "        for model, accuracy in dictionary.items():\n",
        "            # Update the top two models if the current model has a higher accuracy\n",
        "            if accuracy > top_accuracies[0]:\n",
        "                top_models.insert(0, model)\n",
        "                top_accuracies.insert(0, accuracy)\n",
        "            elif accuracy > top_accuracies[1]:\n",
        "                top_models.insert(1, model)\n",
        "                top_accuracies.insert(1, accuracy)\n",
        "\n",
        "    return top_models[:2], top_accuracies[:2]"
      ],
      "metadata": {
        "id": "JbLd2v1CB9NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for accuracy in overall_accuracies:\n",
        "  print(f'Accuracies of non sensitive models with imbalanced data: {accuracy}')\n",
        "\n",
        "best_models, best_accuracies = get_best_classifiers(overall_accuracies)\n",
        "# Print maximum accuracy and corresponding model\n",
        "print(\"\\n\\n2 best accuracies  :\", best_accuracies)\n",
        "print(\"2 best classifiers :\", best_models)"
      ],
      "metadata": {
        "id": "RMtLkDIVA0Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuning following hyperparamter with optuna:**\n",
        "- n_estimators\n",
        "- max_depth\n",
        "- max_features\n",
        "- min_samples_split\n",
        "- min_samples_leaf\n",
        "- criterion"
      ],
      "metadata": {
        "id": "ZrdiLWcFkFb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation of hyperparameters and how they may affect performance:*\n",
        "\n",
        "*   n_estimators:\n",
        ">Increasing number of Decision Trees can improve models perfomance by reducing overfitting, but can also increase training time.\n",
        "\n",
        "* max_depth:\n",
        ">It controls the maximum depth of each decision tree. A deeper tree can capture more complex relationships in the data, but it may also lead to overfitting. Setting an appropriate max_depth value helps balance model complexity and generalization.\n",
        "\n",
        "*   max_features:\n",
        "> It determines the maximum number of features to consider when looking for the best split at each node. Limiting the number of features can improve the diversity and randomness of the trees, reducing the likelihood of overfitting.\n",
        "\n",
        "- min_samples_split:\n",
        "> It specifies the minimum number of samples required to split an internal node during the construction of each decision tree. Increasing this value can prevent overfitting by enforcing a minimum number of samples required for a split.\n",
        "\n",
        "- min_samples_leaf:\n",
        "> It sets the minimum number of samples required to be at a leaf node. Similar to min_samples_split, increasing this value helps prevent overfitting by requiring a minimum number of samples in each leaf.\n",
        "\n",
        "- criterion:\n",
        "> It defines the function used to measure the quality of a split. Gini impurity measures the probability of misclassifying a randomly chosen element, while entropy measures the information gain in terms of the reduction in uncertainty.\n",
        "\n",
        "- learning_rate:\n",
        ">  Determines the contribution of each tree to the final ensemble.\n",
        "Smaller learning rates can lead to better generalization and more robust models, as they prevent overfitting by reducing the impact of individual trees.\n",
        "Higher learning rates can lead to faster convergence and improved training set performance, but they may also increase the risk of overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "GoeLHaUy9Mue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "id": "kIBBy8CT87b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = None\n",
        "best_params = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "def objective(trial):\n",
        "    global best_model\n",
        "    global best_params\n",
        "    global best_accuracy\n",
        "\n",
        "    # Define the hyperparameter search spaces for the first model (Random Forest)\n",
        "    rf_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "    }\n",
        "\n",
        "    rf_model = best_models[0]\n",
        "    rf_model.set_params(**rf_params)\n",
        "\n",
        "\n",
        "    # Define the hyperparameter search spaces for the second model (Gradient Boosting)\n",
        "    gb_params = {\n",
        "    'n_estimators': trial.suggest_int('gb_n_estimators', 100, 500, step=100),\n",
        "    'learning_rate': trial.suggest_loguniform('gb_learning_rate', 0.001, 0.1),\n",
        "    'max_depth': trial.suggest_int('gb_max_depth', 3, 10),\n",
        "    'min_samples_split': trial.suggest_int('gb_min_samples_split', 2, 10),\n",
        "    'min_samples_leaf': trial.suggest_int('gb_min_samples_leaf', 1, 10),\n",
        "    'max_features': trial.suggest_categorical('gb_max_features', ['sqrt', 'log2']),\n",
        "}\n",
        "\n",
        "    gb_model = best_models[1]\n",
        "    gb_model.set_params(**gb_params)\n",
        "\n",
        "    # Train and evaluate both models\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    rf_accuracy = rf_model.score(X_dev, y_dev)\n",
        "    gb_accuracy = gb_model.score(X_dev, y_dev)\n",
        "\n",
        "    # Update the best model and parameters if necessary\n",
        "    if rf_accuracy > best_accuracy:\n",
        "        best_model = rf_model\n",
        "        best_params = rf_params\n",
        "        best_accuracy = rf_accuracy\n",
        "    if gb_accuracy > best_accuracy:\n",
        "        best_model = gb_model\n",
        "        best_params = gb_params\n",
        "        best_accuracy = gb_accuracy\n",
        "\n",
        "    # Return the maximum accuracy of the two models\n",
        "    return max(rf_accuracy, gb_accuracy)\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "id": "cyrR6YnbQdaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of hyperparamter tuning**"
      ],
      "metadata": {
        "id": "5umNd5k2JbR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.visualization import plot_optimization_history, plot_slice, plot_param_importances\n",
        "plot_optimization_history(study)\n",
        "# Visualize the hyperparameter slice\n",
        "plot_slice(study)\n",
        "# Visualize the importance of hyperparameters\n",
        "plot_param_importances(study)"
      ],
      "metadata": {
        "id": "cbd5EnDeI5ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation of model performance after hyperparamter tuning:*\n",
        "\n",
        "> We can clearly see that learning rate is the most important hyperparameter for increasing improvement of model performance, strongly followed by the second most important hyperparamter, max_depth.\n",
        "\n"
      ],
      "metadata": {
        "id": "y3xs_OVyD1wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2) Selecting best classifier**"
      ],
      "metadata": {
        "id": "kszM19buXYRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\\nSelected best model is')\n",
        "print(best_model)"
      ],
      "metadata": {
        "id": "c5SBL59QXFEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on dev-set of best model with best hyperparamters**"
      ],
      "metadata": {
        "id": "7DeaJFnuJYPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refit best model with best hyperparameters\n",
        "best_model.set_params(**best_params)\n",
        "best_model.fit(X_train, y_train)\n",
        "y_pred_dev = best_model.predict(X_dev)\n",
        "cv_scores = cross_val_score(clf, X_train, y_train ,scoring='accuracy', cv=5, n_jobs=-1)\n",
        "print(f'Best Hyperparameters  : {best_params}')\n",
        "eval_model(best_model,cv_scores ,X_train, X_dev, y_dev,accuracies_sensitive_models_ , cv=rep_strat_kfold)"
      ],
      "metadata": {
        "id": "8zeWfqgFGlkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation of model performance after hyperparamter tuning:*\n",
        "\n",
        "> After hyperparameter tuning, the Gradient Boost Classifier achieved the best performance on the dev-set with an overall great increasing in all metrices."
      ],
      "metadata": {
        "id": "68JD-C9kGYyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6) Evaluation on test data**\n"
      ],
      "metadata": {
        "id": "VS-fIo2WkisD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate best model on test set\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "print(f'Final evaluation of {best_model} on test data\\n')\n",
        "print(f'Accuracy on test data              : {accuracy_score(y_test, y_test_pred)}')\n",
        "print(f'Weighted F1-score on test data is  : {f1_score(y_test, y_test_pred ,average= \"weighted\")}')\n",
        "print(\"classification report:\\n\", classification_report(y_test, y_test_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "uJ-KfVGhkqYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final analysis of performance on test-data:**\n",
        "\n",
        "> The best model, GradientBoostingClassifier(learning_rate=0.09759345772168124, max_depth=10,max_features='sqrt', min_samples_leaf=10,min_samples_split=6, n_estimators=500)\n",
        ", was evaluated on the test data. The test-set accuracy of the model was found to be 0.844, indicating that it performed well on real, unseen data.\n",
        "Not surprisingly the highest F1-score has been achieved for class 2, then class 1 and F1-score for class 0 is reasonable as well, which has least number of samples also in test-set\n",
        "Analyzing the classification report, we observe that the model exhibited varying performance for each class on the test data.\n",
        "The performance on the test data aligned with the performance on the dev-set, confirming the generalizability of our model."
      ],
      "metadata": {
        "id": "dCWk0zRB8_dq"
      }
    }
  ]
}